{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 16:49:15.947561: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-18 16:49:16.842515: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /local/java/cuda-11.6.0/lib64/:/local/java/cudnn-linux-x86_64-8.5.0.96_cuda11-archive/lib/:/local/java/cuda-12.2/lib64/:/local/java/cudnn-linux-x86_64-8.9.4.25_cuda12-archive/lib/:/local/java/python-3.11.0/lib:/local/java/postgresql/lib/\n",
      "2024-01-18 16:49:16.842584: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /local/java/cuda-11.6.0/lib64/:/local/java/cudnn-linux-x86_64-8.5.0.96_cuda11-archive/lib/:/local/java/cuda-12.2/lib64/:/local/java/cudnn-linux-x86_64-8.9.4.25_cuda12-archive/lib/:/local/java/python-3.11.0/lib:/local/java/postgresql/lib/\n",
      "2024-01-18 16:49:16.842590: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'eager_lbfgs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, activations\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpolate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m griddata\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meager_lbfgs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lbfgs, Struct\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdoe_lhs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#define size of the network\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'eager_lbfgs'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.io\n",
    "import math\n",
    "import matplotlib.gridspec as gridspec\n",
    "# from plotting import newfig\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import layers, activations\n",
    "from scipy.interpolate import griddata\n",
    "from eager_lbfgs import lbfgs, Struct\n",
    "from doe_lhs import *\n",
    "\n",
    "\n",
    "\n",
    "#define size of the network\n",
    "layer_sizes = [2, 128, 128, 128, 128, 1]\n",
    "\n",
    "sizes_w = []\n",
    "sizes_b = []\n",
    "\n",
    "for i, width in enumerate(layer_sizes):\n",
    "    if i != 1:\n",
    "        sizes_w.append(int(width * layer_sizes[1]))\n",
    "        sizes_b.append(int(width if i != 0 else layer_sizes[1]))\n",
    "\n",
    "\n",
    "#L-BFGS weight getting and setting from https://github.com/pierremtb/PINNs-TF2.0\n",
    "def set_weights(model, w, sizes_w, sizes_b):\n",
    "        for i, layer in enumerate(model.layers[0:]):\n",
    "            start_weights = sum(sizes_w[:i]) + sum(sizes_b[:i])\n",
    "            end_weights = sum(sizes_w[:i+1]) + sum(sizes_b[:i])\n",
    "            weights = w[start_weights:end_weights]\n",
    "            w_div = int(sizes_w[i] / sizes_b[i])\n",
    "            weights = tf.reshape(weights, [w_div, sizes_b[i]])\n",
    "            biases = w[end_weights:end_weights + sizes_b[i]]\n",
    "            weights_biases = [weights, biases]\n",
    "            layer.set_weights(weights_biases)\n",
    "\n",
    "\n",
    "def get_weights(model):\n",
    "        w = []\n",
    "        for layer in model.layers[0:]:\n",
    "            weights_biases = layer.get_weights()\n",
    "            weights = weights_biases[0].flatten()\n",
    "            biases = weights_biases[1]\n",
    "            w.extend(weights)\n",
    "            w.extend(biases)\n",
    "\n",
    "        w = tf.convert_to_tensor(w)\n",
    "        return w\n",
    "\n",
    "#define the neural network model\n",
    "def neural_net(layer_sizes):\n",
    "    model = Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(layer_sizes[0],)))\n",
    "    for width in layer_sizes[1:-1]:\n",
    "        model.add(layers.Dense(\n",
    "            width, activation=tf.nn.tanh,\n",
    "            kernel_initializer=\"glorot_normal\"))\n",
    "    model.add(layers.Dense(\n",
    "            layer_sizes[-1], activation=None,\n",
    "            kernel_initializer=\"glorot_normal\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#define the loss\n",
    "def loss(x_f_batch, t_f_batch,\n",
    "         x0, t0, u0, x_lb,\n",
    "         t_lb, x_ub, t_ub,\n",
    "         col_weights, u_weights):\n",
    "\n",
    "    f_u_pred = f_model(x_f_batch, t_f_batch)\n",
    "    u0_pred = u_model(tf.concat([x0, t0], 1))\n",
    "\n",
    "    u_lb_pred, u_x_lb_pred, = u_x_model(u_model, x_lb, t_lb)\n",
    "    u_ub_pred, u_x_ub_pred, = u_x_model(u_model, x_ub, t_ub)\n",
    "\n",
    "    mse_0_u = tf.reduce_mean(tf.square(u_weights*(u0 - u0_pred)))\n",
    "    mse_b_u = tf.reduce_mean(tf.square(tf.math.subtract(u_lb_pred, u_ub_pred))) + \\\n",
    "              tf.reduce_mean(tf.square(tf.math.subtract(u_x_lb_pred, u_x_ub_pred)))\n",
    "\n",
    "    mse_f_u = tf.reduce_mean(tf.square(col_weights * f_u_pred[0]))\n",
    "\n",
    "    return  mse_0_u + mse_b_u + mse_f_u , tf.reduce_mean(tf.square((u0 - u0_pred))), mse_b_u, tf.reduce_mean(tf.square(f_u_pred))\n",
    "\n",
    "#define the physics-based residual, we want this to be 0\n",
    "\n",
    "@tf.function\n",
    "def f_model(x,t):\n",
    "    u = u_model(tf.concat([x, t],1))\n",
    "    u_x = tf.gradients(u, x)\n",
    "    u_xx = tf.gradients(u_x, x)\n",
    "    u_t = tf.gradients(u,t)\n",
    "    c1 = tf.constant(.0001, dtype = tf.float32)\n",
    "    c2 = tf.constant(5.0, dtype = tf.float32)\n",
    "    f_u = u_t - c1*u_xx + c2*u*u*u - c2*u\n",
    "    return f_u\n",
    "\n",
    "@tf.function\n",
    "def u_x_model(u_model, x, t):\n",
    "    u = u_model(tf.concat([x, t],1))\n",
    "    u_x = tf.gradients(u, x)\n",
    "    return u, u_x\n",
    "\n",
    "@tf.function\n",
    "def grad(model, x_f_batch, t_f_batch, x0_batch, t0_batch, u0_batch, x_lb, t_lb, x_ub, t_ub, col_weights, u_weights):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        loss_value, mse_0, mse_b, mse_f = loss(x_f_batch, t_f_batch, x0_batch, t0_batch, u0_batch, x_lb, t_lb, x_ub, t_ub, col_weights, u_weights)\n",
    "        grads = tape.gradient(loss_value, u_model.trainable_variables)\n",
    "        #print(grads)\n",
    "        grads_col = tape.gradient(loss_value, col_weights)\n",
    "        grads_u = tape.gradient(loss_value, u_weights)\n",
    "        gradients_u = tape.gradient(mse_0, u_model.trainable_variables)\n",
    "        gradients_f = tape.gradient(mse_f, u_model.trainable_variables)\n",
    "\n",
    "    return loss_value, mse_0, mse_b, mse_f, grads, grads_col, grads_u, gradients_u, gradients_f\n",
    "\n",
    "\n",
    "def fit(x_f, t_f, x0, t0, u0, x_lb, t_lb, x_ub, t_ub, col_weights, u_weights, tf_iter, newton_iter):\n",
    "\n",
    "    #Can adjust batch size for collocation points, here we set it to N_f\n",
    "    batch_sz = N_f\n",
    "    n_batches =  N_f // batch_sz\n",
    "\n",
    "    start_time = time.time()\n",
    "    #create optimizer s for the network weights, collocation point mask, and initial boundary mask\n",
    "    tf_optimizer = tf.keras.optimizers.Adam(lr = 0.005, beta_1=.99)\n",
    "    tf_optimizer_weights = tf.keras.optimizers.Adam(lr = 0.005, beta_1=.99)\n",
    "    tf_optimizer_u = tf.keras.optimizers.Adam(lr = 0.005, beta_1=.99)\n",
    "\n",
    "    print(\"starting Adam training\")\n",
    "\n",
    "    # For mini-batch (if used)\n",
    "    for epoch in range(tf_iter):\n",
    "        for i in range(n_batches):\n",
    "\n",
    "            x0_batch = x0\n",
    "            t0_batch = t0\n",
    "            u0_batch = u0\n",
    "\n",
    "            x_f_batch = x_f[i*batch_sz:(i*batch_sz + batch_sz),]\n",
    "            t_f_batch = t_f[i*batch_sz:(i*batch_sz + batch_sz),]\n",
    "\n",
    "            loss_value, mse_0, mse_b, mse_f, grads, grads_col, grads_u, g_u, g_f = grad(u_model, x_f_batch, t_f_batch, x0_batch, t0_batch,  u0_batch, x_lb, t_lb, x_ub, t_ub, col_weights, u_weights)\n",
    "\n",
    "            tf_optimizer.apply_gradients(zip(grads, u_model.trainable_variables))\n",
    "            tf_optimizer_weights.apply_gradients(zip([-grads_col, -grads_u], [col_weights, u_weights]))\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('It: %d, Time: %.2f' % (epoch, elapsed))\n",
    "            tf.print(f\"mse_0: {mse_0}  mse_b  {mse_b}  mse_f: {mse_f}   total loss: {loss_value}\")\n",
    "            start_time = time.time()\n",
    "\n",
    "    #l-bfgs-b optimization\n",
    "    print(\"Starting L-BFGS training\")\n",
    "\n",
    "    loss_and_flat_grad = get_loss_and_flat_grad(x_f_batch, t_f_batch, x0_batch, t0_batch, u0_batch, x_lb, t_lb, x_ub, t_ub, col_weights, u_weights)\n",
    "\n",
    "    lbfgs(loss_and_flat_grad,\n",
    "      get_weights(u_model),\n",
    "      Struct(), maxIter=newton_iter, learningRate=0.8)\n",
    "\n",
    "\n",
    "#L-BFGS implementation from https://github.com/pierremtb/PINNs-TF2.0\n",
    "def get_loss_and_flat_grad(x_f_batch, t_f_batch, x0_batch, t0_batch, u0_batch, x_lb, t_lb, x_ub, t_ub, col_weights, u_weights):\n",
    "    def loss_and_flat_grad(w):\n",
    "        with tf.GradientTape() as tape:\n",
    "            set_weights(u_model, w, sizes_w, sizes_b)\n",
    "            loss_value, _, _, _ = loss(x_f_batch, t_f_batch, x0_batch, t0_batch, u0_batch, x_lb, t_lb, x_ub, t_ub, col_weights, u_weights)\n",
    "        grad = tape.gradient(loss_value, u_model.trainable_variables)\n",
    "        grad_flat = []\n",
    "        for g in grad:\n",
    "            grad_flat.append(tf.reshape(g, [-1]))\n",
    "        grad_flat = tf.concat(grad_flat, 0)\n",
    "        #print(loss_value, grad_flat)\n",
    "        return loss_value, grad_flat\n",
    "\n",
    "    return loss_and_flat_grad\n",
    "\n",
    "\n",
    "def predict(X_star):\n",
    "    X_star = tf.convert_to_tensor(X_star, dtype=tf.float32)\n",
    "    u_star, _ = u_x_model(u_model, X_star[:,0:1],\n",
    "                     X_star[:,1:2])\n",
    "\n",
    "    f_u_star = f_model(X_star[:,0:1],\n",
    "                 X_star[:,1:2])\n",
    "\n",
    "    return u_star.numpy(), f_u_star.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants and weight vectors\n",
    "\n",
    "lb = np.array([-1.0])\n",
    "ub = np.array([1.0])\n",
    "\n",
    "N0 = 512\n",
    "N_b = 100\n",
    "N_f = 20000\n",
    "\n",
    "col_weights = tf.Variable(tf.random.uniform([N_f, 1]))\n",
    "u_weights = tf.Variable(100*tf.random.uniform([N0, 1]))\n",
    "\n",
    "#initialize the NN\n",
    "u_model = neural_net(layer_sizes)\n",
    "\n",
    "#view the NN\n",
    "u_model.summary()\n",
    "\n",
    "\n",
    "# Import data, same data as Raissi et al\n",
    "\n",
    "data = scipy.io.loadmat('AC.mat')\n",
    "\n",
    "t = data['tt'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = data['uu']\n",
    "Exact_u = np.real(Exact)\n",
    "\n",
    "\n",
    "\n",
    "#grab training points from domain\n",
    "idx_x = np.random.choice(x.shape[0], N0, replace=False)\n",
    "x0 = x[idx_x,:]\n",
    "u0 = tf.cast(Exact_u[idx_x,0:1], dtype = tf.float32)\n",
    "\n",
    "idx_t = np.random.choice(t.shape[0], N_b, replace=False)\n",
    "tb = t[idx_t,:]\n",
    "\n",
    "# Grab collocation points using latin hpyercube sampling\n",
    "\n",
    "X_f = lb + (ub-lb)*lhs(2, N_f)\n",
    "\n",
    "x_f = tf.convert_to_tensor(X_f[:,0:1], dtype=tf.float32)\n",
    "t_f = tf.convert_to_tensor(np.abs(X_f[:,1:2]), dtype=tf.float32)\n",
    "\n",
    "\n",
    "X0 = np.concatenate((x0, 0*x0), 1) # (x0, 0)\n",
    "X_lb = np.concatenate((0*tb + lb[0], tb), 1) # (lb[0], tb)\n",
    "X_ub = np.concatenate((0*tb + ub[0], tb), 1) # (ub[0], tb)\n",
    "\n",
    "x0 = tf.cast(X0[:,0:1], dtype = tf.float32)\n",
    "t0 = tf.cast(X0[:,1:2], dtype = tf.float32)\n",
    "\n",
    "x_lb = tf.convert_to_tensor(X_lb[:,0:1], dtype=tf.float32)\n",
    "t_lb = tf.convert_to_tensor(X_lb[:,1:2], dtype=tf.float32)\n",
    "\n",
    "x_ub = tf.convert_to_tensor(X_ub[:,0:1], dtype=tf.float32)\n",
    "t_ub = tf.convert_to_tensor(X_ub[:,1:2], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train loop\n",
    "fit(x_f, t_f, x0, t0, u0, x_lb, t_lb, x_ub, t_ub, col_weights, u_weights, tf_iter = 10000, newton_iter = 10000)\n",
    "\n",
    "\n",
    "#generate meshgrid for forward pass of u_pred\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact_u.T.flatten()[:,None]\n",
    "\n",
    "lb = np.array([-1.0, 0.0])\n",
    "ub = np.array([1.0, 1])\n",
    "\n",
    "u_pred, f_u_pred = predict(X_star)\n",
    "\n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "\n",
    "print('Error u: %e' % (error_u))\n",
    "\n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "\n",
    "FU_pred = griddata(X_star, f_u_pred.flatten(), (X, T), method='cubic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "############################# Plotting ###############################\n",
    "######################################################################\n",
    "\n",
    "X0 = np.concatenate((x0, 0*x0), 1) # (x0, 0)\n",
    "X_lb = np.concatenate((0*tb + lb[0], tb), 1) # (lb[0], tb)\n",
    "X_ub = np.concatenate((0*tb + ub[0], tb), 1) # (ub[0], tb)\n",
    "X_u_train = np.vstack([X0, X_lb, X_ub])\n",
    "\n",
    "# fig, ax = newfig(1.3, 1.0)\n",
    "ax.axis('off')\n",
    "\n",
    "####### Row 0: h(t,x) ##################\n",
    "gs0 = gridspec.GridSpec(1, 2)\n",
    "gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0)\n",
    "ax = plt.subplot(gs0[:, :])\n",
    "\n",
    "h = ax.imshow(U_pred.T, interpolation='nearest', cmap='YlGnBu',\n",
    "              extent=[lb[1], ub[1], lb[0], ub[0]],\n",
    "              origin='lower', aspect='auto')\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "fig.colorbar(h, cax=cax)\n",
    "\n",
    "\n",
    "line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "ax.plot(t[25]*np.ones((2,1)), line, 'k--', linewidth = 1)\n",
    "ax.plot(t[50]*np.ones((2,1)), line, 'k--', linewidth = 1)\n",
    "ax.plot(t[100]*np.ones((2,1)), line, 'k--', linewidth = 1)\n",
    "ax.plot(t[150]*np.ones((2,1)), line, 'k--', linewidth = 1)\n",
    "\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$x$')\n",
    "leg = ax.legend(frameon=False, loc = 'best')\n",
    "#    plt.setp(leg.get_texts(), color='w')\n",
    "ax.set_title('$u(t,x)$', fontsize = 10)\n",
    "\n",
    "####### Row 1: h(t,x) slices ##################\n",
    "gs1 = gridspec.GridSpec(1, 3)\n",
    "gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 0])\n",
    "ax.plot(x,Exact_u[:,50], 'b-', linewidth = 2, label = 'Exact')\n",
    "ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.set_title('$t = %.2f$' % (t[50]), fontsize = 10)\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])\n",
    "\n",
    "ax = plt.subplot(gs1[0, 1])\n",
    "ax.plot(x,Exact_u[:,100], 'b-', linewidth = 2, label = 'Exact')\n",
    "ax.plot(x,U_pred[100,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])\n",
    "ax.set_title('$t = %.2f$' % (t[100]), fontsize = 10)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.3), ncol=5, frameon=False)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 2])\n",
    "ax.plot(x,Exact_u[:,150], 'b-', linewidth = 2, label = 'Exact')\n",
    "ax.plot(x,U_pred[150,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])\n",
    "ax.set_title('$t = %.2f$' % (t[150]), fontsize = 10)\n",
    "\n",
    "#show u_pred across domain\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "h = plt.imshow(U_pred.T, interpolation='nearest', cmap='rainbow',\n",
    "            extent=[0.0, 1.0, -1.0, 1.0],\n",
    "            origin='lower', aspect='auto')\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "fig.colorbar(h, cax=cax)\n",
    "\n",
    "plt.legend(frameon=False, loc = 'best')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ec = plt.imshow(FU_pred.T, interpolation='nearest', cmap='rainbow',\n",
    "            extent=[0.0, math.pi/2, -5.0, 5.0],\n",
    "            origin='lower', aspect='auto')\n",
    "\n",
    "#ax.add_collection(ec)\n",
    "ax.autoscale_view()\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$t$')\n",
    "cbar = plt.colorbar(ec)\n",
    "cbar.set_label('$\\overline{f}_u$ prediction')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(t_f, x_f, c = col_weights.numpy(), s = col_weights.numpy()/10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
