{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31600f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "bc-PINN for Allen Cahn 1D \n",
    "\n",
    "1. With ICGL (10% ADAM Iterations)\n",
    "2. Initialization of parameters obtained from previous segment\n",
    "3. Weighting of loss function with maximum order of derivatives and delta_x\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# %tensorflow_version 1.15\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "from doe_lhs import *\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import errno\n",
    "import math\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34caadad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the class\n",
    "class PhysicsInformedNN:\n",
    "    def __init__(self, x0, u0, tb, X_f, X_star, u_star, layers, lb, ub):\n",
    "        \n",
    "        X0 = np.concatenate((x0, 0*x0), 1) # (x0, 0)\n",
    "        X_lb = np.concatenate((0*tb + lb[0], tb), 1) # (lb[0], tb)\n",
    "        X_ub = np.concatenate((0*tb + ub[0], tb), 1) # (ub[0], tb)\n",
    "        \n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        self.nb = np.size(tb)\n",
    "               \n",
    "        self.x0 = X0[:,0:1]\n",
    "        self.t0 = X0[:,1:2]\n",
    "        self.u0 = u0\n",
    "\n",
    "        self.x_lb = X_lb[:,0:1]\n",
    "        self.t_lb = X_lb[:,1:2]\n",
    "\n",
    "        self.x_ub = X_ub[:,0:1]\n",
    "        self.t_ub = X_ub[:,1:2]\n",
    "        \n",
    "        self.x_f = X_f[:,0:1]\n",
    "        self.t_f = X_f[:,1:2]\n",
    "        \n",
    "        self.x_star = X_star[:,0:1]\n",
    "        self.t_star = X_star[:,2:3]\n",
    "        self.u_star = u_star\n",
    "        \n",
    "        # Initialize NNs\n",
    "        self.layers = layers\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "        \n",
    "        # Initialize NNs\n",
    "        self.wi = tf.constant(1,dtype=tf.float32)\n",
    "        self.wb = tf.constant(1,dtype=tf.float32)\n",
    "        self.wr = tf.constant(1,dtype=tf.float32)\n",
    "        self.delta = tf.constant(1.35, dtype = tf.float32)\n",
    "        \n",
    "        # tf Placeholders        \n",
    "        self.x0_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.x0.shape[1]])\n",
    "        self.t0_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.t0.shape[1]])\n",
    "        \n",
    "        self.u0_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.u0.shape[1]])\n",
    "        \n",
    "        self.x_lb_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.x_lb.shape[1]])\n",
    "        self.t_lb_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.t_lb.shape[1]])\n",
    "        \n",
    "        self.x_ub_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.x_ub.shape[1]])\n",
    "        self.t_ub_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.t_ub.shape[1]])\n",
    "        \n",
    "        self.x_f_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.x_f.shape[1]])\n",
    "        self.t_f_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.t_f.shape[1]])\n",
    "        \n",
    "        self.x_star_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.x_star.shape[1]]) # MODIFIED FOR I.C at all time steps\n",
    "        self.t_star_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.t_star.shape[1]]) # MODIFIED FOR I.C at all time steps\n",
    "        self.u_star_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.u_star.shape[1]]) # MODIFIED FOR I.C at all time steps\n",
    "\n",
    "        \n",
    "        # tf Graphs\n",
    "        self.u0_pred, _ = self.net_uv(self.x0_tf, self.t0_tf)\n",
    "        self.u_lb_pred, self.u_x_lb_pred = self.net_uv(self.x_lb_tf, self.t_lb_tf)\n",
    "        self.u_ub_pred, self.u_x_ub_pred = self.net_uv(self.x_ub_tf, self.t_ub_tf)\n",
    "        self.f_u_pred = self.net_f_uv(self.x_f_tf, self.t_f_tf)\n",
    "        \n",
    "        self.u_star_pred, _ = self.net_uv(self.x_star_tf, self.t_star_tf)\n",
    "        \n",
    "        \n",
    "        # Loss\n",
    "        \n",
    "        self.loss = 64*tf.reduce_mean(tf.square(self.u0_tf - self.u0_pred)) + \\\n",
    "                    tf.reduce_mean(tf.square(self.u_lb_pred - self.u_ub_pred)) + \\\n",
    "                    tf.reduce_mean(tf.square(self.u_x_lb_pred - self.u_x_ub_pred)) + \\\n",
    "                    tf.reduce_mean(tf.square(self.f_u_pred))\n",
    "                   \n",
    "\n",
    "        self.loss_star = tf.reduce_mean(tf.square(self.u_star_tf - self.u_star_pred))\n",
    "                    \n",
    "        self.loss_ui = tf.reduce_mean(tf.square(self.u0_tf - self.u0_pred) + tf.abs(self.u0_tf - self.u0_pred))\n",
    "                       \n",
    "        self.loss_ub = tf.reduce_mean(tf.square(self.u_lb_pred - self.u_ub_pred) + tf.abs(self.u_lb_pred - self.u_ub_pred))+ \\\n",
    "                       tf.reduce_mean(tf.square(self.u_x_lb_pred - self.u_x_ub_pred) + tf.abs(self.u_lb_pred - self.u_ub_pred))\n",
    "                       \n",
    "        self.loss_ur = tf.reduce_mean(tf.square(self.f_u_pred) + tf.abs(self.f_u_pred))\n",
    "\n",
    "        \n",
    "\n",
    "        # Optimizers\n",
    "        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n",
    "                                                                method = 'L-BFGS-B', \n",
    "                                                                options = {'maxiter': 10000,\n",
    "                                                                           'maxfun': 20000,\n",
    "                                                                           'maxcor': 50,\n",
    "                                                                           'maxls': 50,\n",
    "                                                                           'ftol' : 1.0 * np.finfo(float).eps})\n",
    "    \n",
    "        self.initial_learning_rate_star = 0.01\n",
    "        self.initial_learning_rate = 0.01\n",
    "        self.global_step_star = tf.Variable(0,trainable=False)\n",
    "        self.global_step = tf.Variable(0,trainable=False)\n",
    "        \n",
    "        increment_global_step_star = tf.assign(self.global_step_star, self.global_step_star + 1)\n",
    "        increment_global_step = tf.assign(self.global_step, self.global_step + 1)\n",
    "        \n",
    "        decayed_lr_star = tf.train.exponential_decay(self.initial_learning_rate_star,\n",
    "                                                self.global_step_star, 2000,\n",
    "                                                0.1, staircase=True)\n",
    "\n",
    "        decayed_lr = tf.train.exponential_decay(self.initial_learning_rate,\n",
    "                                                self.global_step, 2000,\n",
    "                                                0.2, staircase=True)\n",
    "        \n",
    "        self.optimizer_Adam_star = tf.train.AdamOptimizer(learning_rate=decayed_lr_star)\n",
    "        self.optimizer_Adam = tf.train.AdamOptimizer()\n",
    "        self.train_op_Adam_star = self.optimizer_Adam_star.minimize(self.loss_star,global_step=self.global_step_star)\n",
    "        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
    "                \n",
    "        # tf session\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                                     log_device_placement=False))\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "              \n",
    "    def initialize_NN(self, layers):        \n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers) \n",
    "        for l in range(0,num_layers-1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)        \n",
    "        return weights, biases\n",
    "        \n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]        \n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "        return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "    \n",
    "    def neural_net(self, X, weights, biases):\n",
    "        num_layers = len(weights) + 1\n",
    "        \n",
    "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n",
    "        for l in range(0,num_layers-2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "    \n",
    "    def net_uv(self, x, t):\n",
    "        X = tf.concat([x,t],1)\n",
    "        \n",
    "        uv = self.neural_net(X, self.weights, self.biases)\n",
    "        u = uv[:,0:1]\n",
    "        u_x = tf.gradients(u, x)[0]\n",
    "        return u, u_x\n",
    "\n",
    "    def net_f_uv(self, x, t):\n",
    "        u, u_x = self.net_uv(x,t)\n",
    "        \n",
    "        u_t = tf.gradients(u, t)[0]\n",
    "        u_xx = tf.gradients(u_x,x)[0]\n",
    "        \n",
    "        fu = u_t - 0.0001*u_xx + 5*u*(u-1)*(u+1)\n",
    "        \n",
    "        return fu\n",
    "    \n",
    "    \n",
    "    def callback(self, loss, loss_ui, loss_ub, loss_ur):\n",
    "        self.loss_lbfgs.append(loss)\n",
    "        self.loss_ui_lbfgs.append(loss_ui)\n",
    "        self.loss_ub_lbfgs.append(loss_ub)\n",
    "        self.loss_ur_lbfgs.append(loss_ur)\n",
    "\n",
    "        print('Loss:', loss)\n",
    "        \n",
    "    def train(self, nIter):\n",
    "        \n",
    "        tf_dict = {self.x0_tf: self.x0, self.t0_tf: self.t0,\n",
    "                   self.u0_tf: self.u0, self.x_lb_tf: self.x_lb, \n",
    "                   self.t_lb_tf: self.t_lb, self.x_ub_tf: self.x_ub, \n",
    "                   self.t_ub_tf: self.t_ub, self.x_f_tf: self.x_f, \n",
    "                   self.t_f_tf: self.t_f}\n",
    "        \n",
    "        tf_dict_star = {self.x_star_tf: self.x_star, self.t_star_tf: self.t_star, \n",
    "                        self.u_star_tf: self.u_star}\n",
    "       \n",
    "        self.loss_adam = []\n",
    "        self.loss_ui_adam = []\n",
    "        self.loss_ur_adam = []\n",
    "        self.loss_ub_adam = []\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "        for it in range(nIter):\n",
    "            if it <= nIter/10 :\n",
    "                self.sess.run(self.train_op_Adam_star, tf_dict_star)\n",
    "                if it % 10 == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    loss_value = self.sess.run(self.loss_star, tf_dict_star)\n",
    "                    step_star = self.sess.run(self.global_step_star)\n",
    "                    step = self.sess.run(self.global_step)\n",
    "\n",
    "                    #learning_rate_value = self.sess.run(self.optimizer_Adam._lr)\n",
    "                    self.loss_adam.append(loss_value)\n",
    "                    print('It: %d, Loss: %.3e, Time: %.2f, step_star: %d' % \n",
    "                        (it, loss_value, elapsed, step_star))\n",
    "              \n",
    "              \n",
    "                start_time = time.time()\n",
    "            else:\n",
    "                self.sess.run(self.train_op_Adam, tf_dict)\n",
    "                if it % 10 == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                    loss_value_ui = self.sess.run(self.loss_ui, tf_dict)\n",
    "                    loss_value_ub = self.sess.run(self.loss_ub, tf_dict)\n",
    "                    loss_value_ur = self.sess.run(self.loss_ur, tf_dict)\n",
    "\n",
    "                    self.loss_adam.append(loss_value)\n",
    "                    self.loss_ui_adam.append(loss_value_ui)\n",
    "                    self.loss_ub_adam.append(loss_value_ub)\n",
    "                    self.loss_ur_adam.append(loss_value_ur)\n",
    "\n",
    "                    print('It: %d, Loss: %.3e, Loss_ui: %.3e, Loss_ub: %.3e, Loss_ur: %.3e, Time: %.2f, step_star: %d, step: %d' % \n",
    "                     (it, loss_value, loss_value_ui, loss_value_ub, loss_value_ur, elapsed, step_star, step))\n",
    "                    start_time = time.time()\n",
    "                                          \n",
    "        self.loss_lbfgs = []\n",
    "        self.loss_ui_lbfgs = []\n",
    "        self.loss_ub_lbfgs = []\n",
    "        self.loss_ur_lbfgs = []\n",
    "                                                                             \n",
    "        self.optimizer.minimize(self.sess, \n",
    "                                feed_dict = tf_dict,         \n",
    "                                fetches = [self.loss,self.loss_ui,self.loss_ub,self.loss_ur], \n",
    "                                loss_callback = self.callback)\n",
    "                                    \n",
    "    \n",
    "    def predict(self, X_star):\n",
    "        \n",
    "        tf_dict = {self.x0_tf: X_star[:,0:1], self.t0_tf: X_star[:,1:2]}\n",
    "        \n",
    "        u_star = self.sess.run(self.u0_pred, tf_dict)   \n",
    "             \n",
    "        return u_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50051960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory not created\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_estimator.python.estimator.api._v1.estimator' has no attribute 'opt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 608\u001b[0m\n\u001b[1;32m    605\u001b[0m X_f \u001b[38;5;241m=\u001b[39m temp_lb \u001b[38;5;241m+\u001b[39m  (temp_ub\u001b[38;5;241m-\u001b[39mtemp_lb)\u001b[38;5;241m*\u001b[39mlhs(\u001b[38;5;241m2\u001b[39m,N_f)\n\u001b[1;32m    606\u001b[0m u_star \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(u0,(\u001b[38;5;28mlen\u001b[39m(t),\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 608\u001b[0m \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcounter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mPhysicsInformedNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_star\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_star\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mub\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()                \n\u001b[1;32m    610\u001b[0m \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcounter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtrain(iterations)\n",
      "Cell \u001b[0;32mIn[14], line 89\u001b[0m, in \u001b[0;36mPhysicsInformedNN.__init__\u001b[0;34m(self, x0, u0, tb, X_f, X_star, u_star, layers, lb, ub)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_ur \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39msquare(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_u_pred) \u001b[38;5;241m+\u001b[39m tf\u001b[38;5;241m.\u001b[39mabs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_u_pred))\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Optimizers\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[38;5;241m.\u001b[39mScipyOptimizerInterface(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss, \n\u001b[1;32m     90\u001b[0m                                                         method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL-BFGS-B\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     91\u001b[0m                                                         options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10000\u001b[39m,\n\u001b[1;32m     92\u001b[0m                                                                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxfun\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m20000\u001b[39m,\n\u001b[1;32m     93\u001b[0m                                                                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxcor\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     94\u001b[0m                                                                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxls\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     95\u001b[0m                                                                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mftol\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mfinfo(\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39meps})\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_learning_rate_star \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n",
      "File \u001b[0;32m/local/java/python-ml/22-12-21-python3.9/lib64/python3.9/site-packages/tensorflow/python/util/lazy_loader.py:59\u001b[0m, in \u001b[0;36mLazyLoader.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[1;32m     58\u001b[0m   module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load()\n\u001b[0;32m---> 59\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local/java/python-ml/22-12-21-python3.9/lib64/python3.9/site-packages/tensorflow/python/util/module_wrapper.py:232\u001b[0m, in \u001b[0;36mTFModuleWrapper._getattr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m\"\"\"Imports and caches pre-defined API.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03mWarns if necessary.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03mfails.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 232\u001b[0m   attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tfmw_wrapped_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Placeholder for Google-internal contrib error\u001b[39;00m\n\u001b[1;32m    236\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfmw_public_apis:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow_estimator.python.estimator.api._v1.estimator' has no attribute 'opt'"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "# Initialize the class\n",
    "class bcPhysicsInformedNN:\n",
    "    def __init__(self, x0, u0, t0, u1, tb, X_old, X_f, X_star, u_star, layers, lb, ub, weights, biases):\n",
    "        \n",
    "        X0 = np.concatenate((x0, t0*np.ones(x0.shape)), 1) # (x0, t0)\n",
    "        X_lb = np.concatenate((0*tb + lb[0], tb), 1) # (lb[0], tb)\n",
    "        X_ub = np.concatenate((0*tb + ub[0], tb), 1) # (ub[0], tb)\n",
    "        \n",
    "        \n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        self.nb = np.size(tb)\n",
    "               \n",
    "        self.x0 = X0[:,0:1]\n",
    "        self.t0 = X0[:,1:2]\n",
    "        self.u0 = u0\n",
    "\n",
    "        self.x_lb = X_lb[:,0:1]\n",
    "        self.t_lb = X_lb[:,1:2]\n",
    "\n",
    "        self.x_ub = X_ub[:,0:1]\n",
    "        self.t_ub = X_ub[:,1:2]\n",
    "        \n",
    "        self.x_f = X_f[:,0:1]\n",
    "        self.t_f = X_f[:,1:2]\n",
    "        \n",
    "        self.x_star = X_star[:,0:1]\n",
    "        self.t_star = X_star[:,2:3]\n",
    "\n",
    "        self.u_star = u_star\n",
    "\n",
    "        self.x1 = X_old[:,0:1]\n",
    "        self.t1 = X_old[:,1:2]\n",
    "        \n",
    "        self.u1 = u1\n",
    "\n",
    "        \n",
    "        # Initialize NNs\n",
    "        self.layers = layers\n",
    "        #self.weights, self.biases = self.initialize_NN(layers)\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "        \n",
    "        # Initialize NNs\n",
    "        self.wi = tf.constant(1,dtype=tf.float32)\n",
    "        self.wb = tf.constant(1,dtype=tf.float32)\n",
    "        self.wr = tf.constant(1,dtype=tf.float32)\n",
    "        self.ws = tf.constant(1,dtype=tf.float32)\n",
    "        self.delta = tf.constant(1.35,dtype=tf.float32)\n",
    "        \n",
    "        # tf Placeholders        \n",
    "        self.x0_tf = tf.placeholder(tf.float32, shape=[None, self.x0.shape[1]])\n",
    "        self.t0_tf = tf.placeholder(tf.float32, shape=[None, self.t0.shape[1]])\n",
    "        \n",
    "        self.u0_tf = tf.placeholder(tf.float32, shape=[None, self.u0.shape[1]])\n",
    "        \n",
    "        self.x_lb_tf = tf.placeholder(tf.float32, shape=[None, self.x_lb.shape[1]])\n",
    "        self.t_lb_tf = tf.placeholder(tf.float32, shape=[None, self.t_lb.shape[1]])\n",
    "        \n",
    "        self.x_ub_tf = tf.placeholder(tf.float32, shape=[None, self.x_ub.shape[1]])\n",
    "        self.t_ub_tf = tf.placeholder(tf.float32, shape=[None, self.t_ub.shape[1]])\n",
    "        \n",
    "        self.x_f_tf = tf.placeholder(tf.float32, shape=[None, self.x_f.shape[1]])\n",
    "        self.t_f_tf = tf.placeholder(tf.float32, shape=[None, self.t_f.shape[1]])\n",
    "        \n",
    "        self.x1_tf = tf.placeholder(tf.float32, shape=[None, self.x1.shape[1]])\n",
    "        self.t1_tf = tf.placeholder(tf.float32, shape=[None, self.t1.shape[1]])\n",
    "\n",
    "        self.u1_tf = tf.placeholder(tf.float32, shape=[None, self.u1.shape[1]])\n",
    "        \n",
    "        self.x_star_tf = tf.placeholder(tf.float32, shape=[None, self.x_star.shape[1]]) # MODIFIED FOR I.C at all time steps\n",
    "        self.t_star_tf = tf.placeholder(tf.float32, shape=[None, self.t_star.shape[1]]) # MODIFIED FOR I.C at all time steps\n",
    "        self.u_star_tf = tf.placeholder(tf.float32, shape=[None, self.u_star.shape[1]]) # MODIFIED FOR I.C at all time steps\n",
    "\n",
    "        \n",
    "        # tf Graphs\n",
    "        self.u0_pred, _ = self.net_uv(self.x0_tf, self.t0_tf)\n",
    "        self.u1_pred, _ = self.net_uv(self.x1_tf,self.t1_tf)\n",
    "        self.u_lb_pred, self.u_x_lb_pred = self.net_uv(self.x_lb_tf, self.t_lb_tf)\n",
    "        self.u_ub_pred, self.u_x_ub_pred = self.net_uv(self.x_ub_tf, self.t_ub_tf)\n",
    "        self.f_u_pred = self.net_f_uv(self.x_f_tf, self.t_f_tf)\n",
    "        \n",
    "        self.u_star_pred, _ = self.net_uv(self.x_star_tf, self.t_star_tf)\n",
    "        \n",
    "        \n",
    "        # Loss\n",
    "\n",
    "\n",
    "        self.loss = 64*tf.reduce_mean(tf.square(self.u0_tf - self.u0_pred)) + \\\n",
    "                    tf.reduce_mean(tf.square(self.u_lb_pred - self.u_ub_pred)) + \\\n",
    "                    tf.reduce_mean(tf.square(self.u_x_lb_pred - self.u_x_ub_pred)) + \\\n",
    "                    tf.reduce_mean(tf.square(self.f_u_pred)) + \\\n",
    "                    64*tf.reduce_mean(tf.square(self.u1_pred - self.u1_tf))\n",
    "        \n",
    "\n",
    "\n",
    "        self.loss_star = tf.reduce_mean(tf.square(self.u_star_tf - self.u_star_pred))\n",
    "                    \n",
    "        self.loss_ui = tf.reduce_mean(tf.square(self.u0_tf - self.u0_pred) + tf.abs(self.u0_tf - self.u0_pred))\n",
    "                       \n",
    "        self.loss_ub = tf.reduce_mean(tf.square(self.u_lb_pred - self.u_ub_pred) + tf.abs(self.u_lb_pred - self.u_ub_pred))+ \\\n",
    "                       tf.reduce_mean(tf.square(self.u_x_lb_pred - self.u_x_ub_pred) + tf.abs(self.u_lb_pred - self.u_ub_pred))\n",
    "                       \n",
    "        self.loss_ur = tf.reduce_mean(tf.square(self.f_u_pred) + tf.abs(self.f_u_pred))\n",
    "        self.loss_us = tf.reduce_mean(tf.square(self.u1_pred - self.u1_tf) + tf.abs(self.u1_pred - self.u1_tf))\n",
    "\n",
    "        self.loss_hui = tf.abs(self.u0_tf - self.u0_pred)\n",
    "                       \n",
    "        self.loss_hub = tf.abs(self.u_lb_pred - self.u_ub_pred)+ \\\n",
    "                       tf.abs(self.u_x_lb_pred - self.u_x_ub_pred)\n",
    "                       \n",
    "        self.loss_hur = tf.abs(self.f_u_pred)\n",
    "\n",
    "        self.loss_hus = tf.abs(self.u1_pred - self.u1_tf)\n",
    "\n",
    "\n",
    "        \n",
    "        # Optimizers\n",
    "        self.optimizer = tf.tf.compat.v1.estimator.opt.ScipyOptimizerInterface(self.loss, \n",
    "                                                                method = 'L-BFGS-B', \n",
    "                                                                options = {'maxiter': 10000,\n",
    "                                                                           'maxfun': 20000,\n",
    "                                                                           'maxcor': 50,\n",
    "                                                                           'maxls': 50,\n",
    "                                                                           'ftol' : 1.0 * np.finfo(float).eps})\n",
    "    \n",
    "        self.initial_learning_rate_star = 0.01\n",
    "        self.initial_learning_rate = 0.01\n",
    "        self.global_step_star = tf.Variable(0,trainable=False)\n",
    "        self.global_step = tf.Variable(0,trainable=False)\n",
    "        \n",
    "        increment_global_step_star = tf.assign(self.global_step_star, self.global_step_star + 1)\n",
    "        increment_global_step = tf.assign(self.global_step, self.global_step + 1)\n",
    "        \n",
    "        decayed_lr_star = tf.train.exponential_decay(self.initial_learning_rate_star,\n",
    "                                                self.global_step_star, 2000,\n",
    "                                                0.1, staircase=True)\n",
    "\n",
    "        decayed_lr = tf.train.exponential_decay(self.initial_learning_rate,\n",
    "                                                self.global_step, 2000,\n",
    "                                                0.2, staircase=True)\n",
    "        \n",
    "        self.optimizer_Adam_star = tf.train.AdamOptimizer(learning_rate=decayed_lr_star)\n",
    "        self.optimizer_Adam = tf.train.AdamOptimizer()\n",
    "        self.train_op_Adam_star = self.optimizer_Adam_star.minimize(self.loss_star,global_step=self.global_step_star)\n",
    "        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
    "                \n",
    "        # tf session\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                                     log_device_placement=False))\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    def initialize_NN(self, layers):        \n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers) \n",
    "        for l in range(0,num_layers-1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)        \n",
    "        return weights, biases\n",
    "        \n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]        \n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "        return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "    \n",
    "    def neural_net(self, X, weights, biases):\n",
    "        num_layers = len(weights) + 1\n",
    "        \n",
    "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n",
    "        for l in range(0,num_layers-2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "    \n",
    "    def net_uv(self, x, t):\n",
    "        X = tf.concat([x,t],1)\n",
    "        \n",
    "        uv = self.neural_net(X, self.weights, self.biases)\n",
    "        u = uv[:,0:1]\n",
    "        u_x = tf.gradients(u, x)[0]\n",
    "        return u, u_x\n",
    "\n",
    "    def net_f_uv(self, x, t):\n",
    "        u, u_x = self.net_uv(x,t)\n",
    "        \n",
    "        u_t = tf.gradients(u, t)[0]\n",
    "        u_xx = tf.gradients(u_x,x)[0]\n",
    "        \n",
    "        fu = u_t - 0.0001*u_xx + 5*u*(u-1)*(u+1)\n",
    "        return fu\n",
    "    \n",
    "    def callback(self, loss, loss_ui, loss_ub, loss_ur, loss_us):\n",
    "        self.loss_lbfgs.append(loss)\n",
    "        self.loss_ui_lbfgs.append(loss_ui)\n",
    "        self.loss_ub_lbfgs.append(loss_ub)\n",
    "        self.loss_ur_lbfgs.append(loss_ur)\n",
    "        self.loss_us_lbfgs.append(loss_us)\n",
    "\n",
    "        print('Loss:', loss)\n",
    "\n",
    "    def train(self, nIter):\n",
    "        \n",
    "        tf_dict = {self.x0_tf: self.x0, self.t0_tf: self.t0,\n",
    "                   self.u0_tf: self.u0, self.x_lb_tf: self.x_lb, \n",
    "                   self.t_lb_tf: self.t_lb, self.x_ub_tf: self.x_ub, \n",
    "                   self.t_ub_tf: self.t_ub, self.x_f_tf: self.x_f, \n",
    "                   self.t_f_tf: self.t_f, self.x1_tf: self.x1, \n",
    "                   self.t1_tf: self.t1, self.u1_tf: self.u1}\n",
    "       \n",
    "        tf_dict_star = {self.x_star_tf: self.x_star, self.t_star_tf: self.t_star, \n",
    "                        self.u_star_tf: self.u_star}\n",
    "       \n",
    "        self.loss_adam = []\n",
    "        self.loss_ui_adam = []\n",
    "        self.loss_ub_adam = []\n",
    "        self.loss_ur_adam = []\n",
    "        self.loss_us_adam = []\n",
    "\n",
    "        start_time = time.time()\n",
    "        for it in range(nIter):\n",
    "            \n",
    "            \n",
    "            \n",
    "            if it <= nIter/10 :\n",
    "                self.sess.run(self.train_op_Adam_star, tf_dict_star)\n",
    "                if it % 10 == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    loss_value = self.sess.run(self.loss_star, tf_dict_star)\n",
    "                    self.loss_adam.append(loss_value)\n",
    "                    print('It: %d, Loss: %.3e, Time: %.2f' % \n",
    "                        (it, loss_value, elapsed))\n",
    "                    start_time = time.time()\n",
    "            else:\n",
    "                self.sess.run(self.train_op_Adam, tf_dict)\n",
    "                if it % 10 == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                    loss_value_ui = self.sess.run(self.loss_ui, tf_dict)\n",
    "                    loss_value_ub = self.sess.run(self.loss_ub, tf_dict)\n",
    "                    loss_value_ur = self.sess.run(self.loss_ur, tf_dict)\n",
    "                    loss_value_us = self.sess.run(self.loss_us, tf_dict)\n",
    "\n",
    "                    self.loss_adam.append(loss_value)\n",
    "                    self.loss_ui_adam.append(loss_value_ui)\n",
    "                    self.loss_ub_adam.append(loss_value_ub)\n",
    "                    self.loss_ur_adam.append(loss_value_ur)\n",
    "                    self.loss_us_adam.append(loss_value_us)\n",
    "\n",
    "                    print('It: %d, Loss: %.3e, Loss_ui: %.3e, Loss_ub: %.3e, Loss_ur: %.3e, Loss_us: %.3e, Time: %.2f' % \n",
    "                     (it, loss_value, loss_value_ui, loss_value_ub, loss_value_ur, loss_value_us, elapsed))\n",
    "                    start_time = time.time()\n",
    "                                          \n",
    "        self.loss_lbfgs = []\n",
    "        self.loss_ui_lbfgs = []\n",
    "        self.loss_ub_lbfgs = []\n",
    "        self.loss_ur_lbfgs = []\n",
    "        self.loss_us_lbfgs = []  \n",
    "                                                                       \n",
    "        self.optimizer.minimize(self.sess, \n",
    "                                feed_dict = tf_dict,         \n",
    "                                fetches = [self.loss, self.loss_ui, self.loss_ub, self.loss_ur, self.loss_us], \n",
    "                                loss_callback = self.callback)\n",
    "                                    \n",
    "    def predict(self, X_star):\n",
    "        \n",
    "        tf_dict = {self.x0_tf: X_star[:,0:1], self.t0_tf: X_star[:,1:2]}\n",
    "        \n",
    "        u_star = self.sess.run(self.u0_pred, tf_dict)   \n",
    "\n",
    "        return u_star\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8375e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\": \n",
    "     \n",
    "    noise = 0.0        \n",
    "\n",
    "    #layers = [2, 200, 200, 200, 200, 1]\n",
    "    #layers = [2, 256, 256, 256, 256, 1]\n",
    "    layers = [2, 128, 128, 128, 128, 128, 128, 1]\n",
    "    path = './AC_C1_0001_ICGL_TL'\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError as e:\n",
    "        if e.errno == errno.EEXIST:\n",
    "            print('Directory not created')\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "        \n",
    "    data = scipy.io.loadmat('./AC_R_1.mat')\n",
    "    \n",
    "    t = data['tt'].flatten()[:,None]\n",
    "    #x = data['x'].flatten()[:,None]\n",
    "    Exact = data['uu']\n",
    "    Exact_u = np.real(Exact)\n",
    "    Nx = 128\n",
    "    x = np.linspace(-1,1,Nx)\n",
    "    x = x.flatten()[:,None]\n",
    "    dx = x[2] - x[1]\n",
    "    \n",
    "    x0 = x\n",
    "    u0 = (x**2)*np.cos(np.pi*x)\n",
    "    #u0 = Exact_u[:,0]\n",
    "    u0 = u0.reshape(x.shape)\n",
    "    \n",
    "\n",
    "    start  = 0\n",
    "    step = 50\n",
    "    stop = 201\n",
    "    steps_lb = np.arange(0,stop+step,step)\n",
    "    steps_ub = 1 + steps_lb\n",
    "    \n",
    "    iterations = 10000\n",
    "    N_f = 20000 \n",
    "    counter = 0\n",
    "\n",
    "    for i in range(0,steps_lb.size-1):\n",
    "        t1 = steps_lb[i]\n",
    "        t2 = steps_ub[i+1]\n",
    "        temp_t = t[:t2,:]\n",
    "        t = t[t1:t2,:]\n",
    "        \n",
    "        ############### For implementing ICGL ##############\n",
    "        X, T = np.meshgrid(x,t)\n",
    "        X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "        \n",
    "        ############### Modified only 50% of the total segment size##############\n",
    "        N_b = int(step/2)\n",
    "        idx_t = np.random.choice(t.shape[0], N_b, replace=False)\n",
    "        tb = t[idx_t,:]\n",
    "                    \n",
    "        \n",
    "        if counter == 0:\n",
    "            # Domain bounds\n",
    "            lb = np.array([-1, 0.0])\n",
    "            ub = np.array([1, np.max(t)])\n",
    "            \n",
    "            temp_lb = np.array([-1, np.min(t)])\n",
    "            temp_ub = np.array([1, np.max(t)])\n",
    "            \n",
    "            X_f = temp_lb +  (temp_ub-temp_lb)*lhs(2,N_f)\n",
    "            u_star = np.tile(u0,(len(t),1))\n",
    "            \n",
    "            globals()[f\"model_{counter}\"] = PhysicsInformedNN(x0, u0, tb, X_f, X_star, u_star, layers, lb, ub)\n",
    "            start_time = time.time()                \n",
    "            globals()[f\"model_{counter}\"].train(iterations)\n",
    "            elapsed = time.time() - start_time                \n",
    "            print('Training time: %.4f' % (elapsed))\n",
    "            x_old, t_old = np.meshgrid(x,temp_t)\n",
    "            X_old = np.hstack((x_old.flatten()[:,None], t_old.flatten()[:,None]))\n",
    "            pred = globals()[f\"model_{counter}\"].predict(X_old)\n",
    "            u1 = pred\n",
    "        else:\n",
    "            # Domain bounds\n",
    "            # For any step other than initial step\n",
    "            lb = np.array([-1, 0.0])\n",
    "            ub = np.array([1, np.max(t)])\n",
    "            \n",
    "            temp_lb = np.array([-1, np.min(t)])\n",
    "            temp_ub = np.array([1, np.max(t)])\n",
    "            \n",
    "            u0 = u1[-Nx:]\n",
    "            t0 = t[0]\n",
    "            \n",
    "            X_f = temp_lb + (temp_ub - temp_lb)*lhs(2,N_f)\n",
    "            u_star = np.tile(u0,(len(t),1))\n",
    "            \n",
    "            globals()[f\"model_{counter}\"] = bcPhysicsInformedNN(x0, u0, t0, u1, tb, X_old, X_f, X_star, u_star, layers, lb, ub, W, b)\n",
    "            start_time = time.time()                \n",
    "            globals()[f\"model_{counter}\"].train(iterations)\n",
    "            elapsed = time.time() - start_time                \n",
    "            print('Training time: %.4f' % (elapsed))\n",
    "            x_old, t_old = np.meshgrid(x,temp_t)\n",
    "            X_old = np.hstack((x_old.flatten()[:,None], t_old.flatten()[:,None]))\n",
    "            pred = globals()[f\"model_{counter}\"].predict(X_old)\n",
    "            u1 = pred\n",
    "            \n",
    "                                   \n",
    "        layers_freeze = 2\n",
    "        weights = globals()[f\"model_{counter}\"].sess.run(globals()[f\"model_{counter}\"].weights)\n",
    "        biases = globals()[f\"model_{counter}\"].sess.run(globals()[f\"model_{counter}\"].biases)\n",
    "        W = [] \n",
    "        b = []\n",
    "        for i in range(len(weights)):\n",
    "            if i < layers_freeze:\n",
    "                W.append(tf.Variable(weights[i],trainable='False'))\n",
    "                b.append(tf.Variable(biases[i],trainable='False'))\n",
    "            else:\n",
    "                W.append(tf.Variable(weights[i],trainable='True'))\n",
    "                b.append(tf.Variable(biases[i],trainable='True'))\n",
    "        \n",
    "        \n",
    "        t = data['tt'].flatten()[:,None]\n",
    "        \n",
    "        \n",
    "        os.chdir(path)\n",
    "        name = 'Sequential_training_' + str(t1) + '_' + str(t2)\n",
    "        path1 = path + '/' + name\n",
    "        os.makedirs(name)\n",
    "        os.chdir(path1)\n",
    "        #saver = tf.train.Saver()\n",
    "        #saver.save( globals()[f\"model_{counter}\"].sess,'model')\n",
    "\n",
    "        file_name = \"biases.pkl\"\n",
    "        open_file = open(file_name, \"wb\")\n",
    "        pickle.dump(biases, open_file)\n",
    "        open_file.close()\n",
    "\n",
    "        file_name = \"weights.pkl\"\n",
    "        open_file = open(file_name, \"wb\")\n",
    "        pickle.dump(weights, open_file)\n",
    "        open_file.close()\n",
    "\n",
    "        file_name = \"u1.pkl\"\n",
    "        open_file = open(file_name, \"wb\")\n",
    "        pickle.dump(u1, open_file)\n",
    "        open_file.close()\n",
    "        \n",
    "        file_name = \"MSE_ADAM.pkl\"\n",
    "        open_file = open(file_name, \"wb\")\n",
    "        pickle.dump(globals()[f\"model_{counter}\"].loss_adam, open_file)\n",
    "        open_file.close()\n",
    "\n",
    "        file_name = \"MSE_LBFGS.pkl\"\n",
    "        open_file = open(file_name, \"wb\")\n",
    "        pickle.dump(globals()[f\"model_{counter}\"].loss_lbfgs, open_file)\n",
    "        open_file.close()\n",
    "\n",
    "        file_name = \"MSE_UI_ADAM.pkl\"\n",
    "        open_file = open(file_name, \"wb\")\n",
    "        pickle.dump(globals()[f\"model_{counter}\"].loss_ui_adam, open_file)\n",
    "        open_file.close()\n",
    "\n",
    "        file_name = \"MSE_UB_ADAM.pkl\"\n",
    "        open_file = open(file_name, \"wb\")\n",
    "        pickle.dump(globals()[f\"model_{counter}\"].loss_ub_adam, open_file)\n",
    "        open_file.close()\n",
    "\n",
    "        file_name = \"MSE_UR_ADAM.pkl\"\n",
    "        open_file = open(file_name, \"wb\")\n",
    "        pickle.dump(globals()[f\"model_{counter}\"].loss_ur_adam, open_file)\n",
    "        open_file.close()\n",
    "\n",
    "        file_name = \"MSE_UI_LBFGS.pkl\"\n",
    "        open_file = open(file_name, \"wb\")\n",
    "        pickle.dump(globals()[f\"model_{counter}\"].loss_ui_lbfgs, open_file)\n",
    "        open_file.close()\n",
    "\n",
    "        file_name = \"MSE_UB_LBFGS.pkl\"\n",
    "        open_file = open(file_name, \"wb\")\n",
    "        pickle.dump(globals()[f\"model_{counter}\"].loss_ub_lbfgs, open_file)\n",
    "        open_file.close()\n",
    "\n",
    "        file_name = \"MSE_UR_LBFGS.pkl\"\n",
    "        open_file = open(file_name, \"wb\")\n",
    "        pickle.dump(globals()[f\"model_{counter}\"].loss_ur_lbfgs, open_file)\n",
    "        open_file.close()\n",
    "\n",
    "\n",
    "        if counter >= 1:\n",
    "\n",
    "          file_name = \"MSE_US_ADAM.pkl\"\n",
    "          open_file = open(file_name, \"wb\")\n",
    "          pickle.dump(globals()[f\"model_{counter}\"].loss_us_adam, open_file)\n",
    "          open_file.close()\n",
    "          \n",
    "          file_name = \"MSE_US_LBFGS.pkl\"\n",
    "          open_file = open(file_name, \"wb\")\n",
    "          pickle.dump(globals()[f\"model_{counter}\"].loss_us_lbfgs, open_file)\n",
    "          open_file.close()\n",
    "\n",
    "\n",
    "        os.chdir(path)\n",
    "\n",
    "        counter += 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e3a52e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
