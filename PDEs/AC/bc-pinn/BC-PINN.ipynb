{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94082a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../Utilities/')\n",
    "\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "from doe_lhs import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "# from plotting import newfig, savefig\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "import time\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40bf8777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CUDA support \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a575e8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the deep neural network\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__()\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        # set up layer order dict\n",
    "        self.activation = torch.nn.Tanh\n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2adebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the physics-guided neural network\n",
    "class PhysicsInformedNN():\n",
    "    def __init__(self,x_u, y_u, x_f,x_lb,x_ub,X_u_ts ,u_ts , net, device):\n",
    "        super(PhysicsInformedNN, self).__init__()\n",
    "        \n",
    "        self.batch_size = 10000000\n",
    "        self.device = device\n",
    "        shuffle = True\n",
    "\n",
    "        self.train_x_u = torch.tensor(x_u, requires_grad=True).float().to(self.device)\n",
    "        self.train_y_u = torch.tensor(y_u, requires_grad=True).float().to(self.device)\n",
    "        self.train_x_f = torch.tensor(x_f, requires_grad=True).float().to(self.device)\n",
    "\n",
    "        self.train_x_lb = torch.tensor(x_lb, requires_grad=True).float().to(device)\n",
    "        self.train_x_ub = torch.tensor(x_ub, requires_grad=True).float().to(device) \n",
    "            \n",
    "    \n",
    "        # deep neural networks\n",
    "        self.dnn = net\n",
    "\n",
    "        # Adam optimizer\n",
    "        self.Adam_optim = torch.optim.Adam(\n",
    "            self.dnn.parameters(),\n",
    "            lr = 1e-4,\n",
    "            betas = (0.9, 0.999)\n",
    "        )\n",
    "        # L-BFGS optimizer\n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "            self.dnn.parameters(), \n",
    "            lr=1.0, \n",
    "            max_iter=50000, \n",
    "            max_eval=50000, \n",
    "            history_size=50,\n",
    "            tolerance_grad=1e-5, \n",
    "            tolerance_change=1e-9,\n",
    "            line_search_fn=\"strong_wolfe\"       # can be \"strong_wolfe\"\n",
    "        )\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            list(zip(self.train_x_u,self.train_y_u)), batch_size=self.batch_size, shuffle=shuffle\n",
    "        )\n",
    "        \n",
    "        self.iter = 0\n",
    "    \n",
    "    def get_residual(self,X):\n",
    "        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(self.device)\n",
    "        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(self.device)\n",
    "\n",
    "        u = self.net_u(x, t)\n",
    "        f = self.net_f(x, t, u)\n",
    "        return u, f\n",
    "    \n",
    "    \n",
    "    def net_u(self, x, t):  \n",
    "        u = self.dnn(torch.cat([x, t], dim=1))\n",
    "        return u\n",
    "    \n",
    "    def boundary_loss(self):\n",
    "        x_lb = torch.tensor(self.train_x_lb[:, 0:1], requires_grad=True).float().to(self.device)\n",
    "        t_lb = torch.tensor(self.train_x_lb[:, 1:2], requires_grad=True).float().to(self.device)\n",
    "\n",
    "        x_ub = torch.tensor(self.train_x_ub[:, 0:1], requires_grad=True).float().to(self.device)\n",
    "        t_ub = torch.tensor(self.train_x_ub[:, 1:2], requires_grad=True).float().to(self.device)\n",
    "        \n",
    "        y_pred_lb = self.dnn.forward(torch.cat([x_lb, t_lb], dim=1))\n",
    "        y_pred_ub = self.dnn.forward(torch.cat([x_ub, t_ub], dim=1))\n",
    "        \n",
    "        u_lb = y_pred_lb[:,0:1]\n",
    "        u_ub = y_pred_ub[:,0:1]\n",
    "\n",
    "        u_lb_x = torch.autograd.grad(\n",
    "                u_lb, x_lb, \n",
    "                grad_outputs=torch.ones_like(u_lb),\n",
    "                retain_graph=True,\n",
    "                create_graph=True\n",
    "            )[0]\n",
    "\n",
    "        u_ub_x = torch.autograd.grad(\n",
    "                u_ub, x_ub, \n",
    "                grad_outputs=torch.ones_like(u_ub),\n",
    "                retain_graph=True,\n",
    "                create_graph=True\n",
    "            )[0]\n",
    "\n",
    "        loss = torch.mean((y_pred_lb - y_pred_ub)**2) + \\\n",
    "               torch.mean((u_lb_x - u_ub_x)**2)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def net_f(self, x, t, u):\n",
    "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
    "        \n",
    "        u_t = torch.autograd.grad(\n",
    "            u, t, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_x = torch.autograd.grad(\n",
    "            u, x, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_xx = torch.autograd.grad(\n",
    "            u_x, x, \n",
    "            grad_outputs=torch.ones_like(u_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "\n",
    "        f = u_t - 0.0001* u_xx  + 5*u*u*u - 5*u \n",
    "        return f\n",
    "    \n",
    "    def loss_func(self):\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        x = self.train_x_u\n",
    "        y = self.train_y_u\n",
    "\n",
    "        y_pred, _ = self.get_residual(x)\n",
    "        _, f_u  = self.get_residual(self.train_x_f)\n",
    "\n",
    "        loss_u = torch.mean((y - y_pred) ** 2)\n",
    "        loss_f = torch.mean(f_u ** 2)\n",
    "\n",
    "        b_loss = self.boundary_loss()\n",
    "\n",
    "        loss = loss_u + loss_f + b_loss\n",
    "        loss.backward()\n",
    "\n",
    "        self.iter += 1\n",
    "        if self.iter % 100 == 0:\n",
    "            u_pred, f_pred = model.predict(self.X)\n",
    "            error_u = np.linalg.norm(self.U-u_pred,2)/np.linalg.norm(self.U,2)\n",
    "            print('Iter %d, Error u: %e' % (self.iter, error_u)) \n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def train(self,epoch,X,U):\n",
    "\n",
    "        self.train_1(epoch,X,U)\n",
    "        self.train_2()\n",
    "    \n",
    "    def train_1(self,epoch,X,U):\n",
    "        \n",
    "        self.X = X\n",
    "        self.U = U\n",
    "\n",
    "        for e in range(epoch):\n",
    "            for i, (x, y) in enumerate(self.train_loader):\n",
    "                self.dnn.train()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                y_pred, _ = self.get_residual(x)\n",
    "                _, f_u  = self.get_residual(self.train_x_f)\n",
    "\n",
    "                loss_u = torch.mean((y - y_pred) ** 2)\n",
    "                loss_f = torch.mean(f_u ** 2)\n",
    "\n",
    "                b_loss = self.boundary_loss()\n",
    "\n",
    "                loss = 100 * loss_u + loss_f + b_loss\n",
    "                loss.backward(retain_graph = True)\n",
    "                self.Adam_optim.step()\n",
    "\n",
    "            self.iter += 1\n",
    "            if self.iter % 100 == 0:\n",
    "                u_pred, f_pred = model.predict(X)\n",
    "                error_u = np.linalg.norm(U-u_pred,2)/np.linalg.norm(U,2)\n",
    "                print('Iter %d, Error u: %e' % (self.iter, error_u)) \n",
    "\n",
    "\n",
    "    def train_2(self):\n",
    "        self.dnn.train()\n",
    "        self.optimizer.step(self.loss_func)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
    "        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval()\n",
    "        u = self.net_u(x, t)\n",
    "        f = self.net_f(x, t, u)\n",
    "        u = u.detach().cpu().numpy()\n",
    "        f = f.detach().cpu().numpy()\n",
    "        return u, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdf7c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the physics-guided neural network\n",
    "class BC_PhysicsInformedNN():\n",
    "    def __init__(self,x_u, y_u, x_f,x_lb,x_ub,X_u_ts ,u_ts , net, device):\n",
    "        super(PhysicsInformedNN, self).__init__()\n",
    "        \n",
    "        self.batch_size = 10000000\n",
    "        self.device = device\n",
    "        shuffle = True\n",
    "\n",
    "        self.train_x_u = torch.tensor(x_u, requires_grad=True).float().to(self.device)\n",
    "        self.train_y_u = torch.tensor(y_u, requires_grad=True).float().to(self.device)\n",
    "        self.train_x_f = torch.tensor(x_f, requires_grad=True).float().to(self.device)\n",
    "\n",
    "        self.train_x_lb = torch.tensor(x_lb, requires_grad=True).float().to(device)\n",
    "        self.train_x_ub = torch.tensor(x_ub, requires_grad=True).float().to(device) \n",
    "            \n",
    "    \n",
    "        # deep neural networks\n",
    "        self.dnn = net\n",
    "\n",
    "        # Adam optimizer\n",
    "        self.Adam_optim = torch.optim.Adam(\n",
    "            self.dnn.parameters(),\n",
    "            lr = 1e-4,\n",
    "            betas = (0.9, 0.999)\n",
    "        )\n",
    "        # L-BFGS optimizer\n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "            self.dnn.parameters(), \n",
    "            lr=1.0, \n",
    "            max_iter=50000, \n",
    "            max_eval=50000, \n",
    "            history_size=50,\n",
    "            tolerance_grad=1e-5, \n",
    "            tolerance_change=1e-9,\n",
    "            line_search_fn=\"strong_wolfe\"       # can be \"strong_wolfe\"\n",
    "        )\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            list(zip(self.train_x_u,self.train_y_u)), batch_size=self.batch_size, shuffle=shuffle\n",
    "        )\n",
    "        \n",
    "        self.iter = 0\n",
    "    \n",
    "    def get_residual(self,X):\n",
    "        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(self.device)\n",
    "        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(self.device)\n",
    "\n",
    "        u = self.net_u(x, t)\n",
    "        f = self.net_f(x, t, u)\n",
    "        return u, f\n",
    "    \n",
    "    \n",
    "    def net_u(self, x, t):  \n",
    "        u = self.dnn(torch.cat([x, t], dim=1))\n",
    "        return u\n",
    "    \n",
    "    def boundary_loss(self):\n",
    "        x_lb = torch.tensor(self.train_x_lb[:, 0:1], requires_grad=True).float().to(self.device)\n",
    "        t_lb = torch.tensor(self.train_x_lb[:, 1:2], requires_grad=True).float().to(self.device)\n",
    "\n",
    "        x_ub = torch.tensor(self.train_x_ub[:, 0:1], requires_grad=True).float().to(self.device)\n",
    "        t_ub = torch.tensor(self.train_x_ub[:, 1:2], requires_grad=True).float().to(self.device)\n",
    "        \n",
    "        y_pred_lb = self.dnn.forward(torch.cat([x_lb, t_lb], dim=1))\n",
    "        y_pred_ub = self.dnn.forward(torch.cat([x_ub, t_ub], dim=1))\n",
    "        \n",
    "        u_lb = y_pred_lb[:,0:1]\n",
    "        u_ub = y_pred_ub[:,0:1]\n",
    "\n",
    "        u_lb_x = torch.autograd.grad(\n",
    "                u_lb, x_lb, \n",
    "                grad_outputs=torch.ones_like(u_lb),\n",
    "                retain_graph=True,\n",
    "                create_graph=True\n",
    "            )[0]\n",
    "\n",
    "        u_ub_x = torch.autograd.grad(\n",
    "                u_ub, x_ub, \n",
    "                grad_outputs=torch.ones_like(u_ub),\n",
    "                retain_graph=True,\n",
    "                create_graph=True\n",
    "            )[0]\n",
    "\n",
    "        loss = torch.mean((y_pred_lb - y_pred_ub)**2) + \\\n",
    "               torch.mean((u_lb_x - u_ub_x)**2)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def net_f(self, x, t, u):\n",
    "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
    "        \n",
    "        u_t = torch.autograd.grad(\n",
    "            u, t, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_x = torch.autograd.grad(\n",
    "            u, x, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_xx = torch.autograd.grad(\n",
    "            u_x, x, \n",
    "            grad_outputs=torch.ones_like(u_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "\n",
    "        f = u_t - 0.0001* u_xx  + 5*u*u*u - 5*u \n",
    "        return f\n",
    "    \n",
    "    def loss_func(self):\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        x = self.train_x_u\n",
    "        y = self.train_y_u\n",
    "\n",
    "        y_pred, _ = self.get_residual(x)\n",
    "        _, f_u  = self.get_residual(self.train_x_f)\n",
    "\n",
    "        loss_u = torch.mean((y - y_pred) ** 2)\n",
    "        loss_f = torch.mean(f_u ** 2)\n",
    "\n",
    "        b_loss = self.boundary_loss()\n",
    "\n",
    "        loss = loss_u + loss_f + b_loss\n",
    "        loss.backward()\n",
    "\n",
    "        self.iter += 1\n",
    "        if self.iter % 100 == 0:\n",
    "            u_pred, f_pred = model.predict(self.X)\n",
    "            error_u = np.linalg.norm(self.U-u_pred,2)/np.linalg.norm(self.U,2)\n",
    "            print('Iter %d, Error u: %e' % (self.iter, error_u)) \n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def train(self,epoch,X,U):\n",
    "\n",
    "        self.train_1(epoch,X,U)\n",
    "        self.train_2()\n",
    "    \n",
    "    def train_1(self,epoch,X,U):\n",
    "        \n",
    "        self.X = X\n",
    "        self.U = U\n",
    "\n",
    "        for e in range(epoch):\n",
    "            for i, (x, y) in enumerate(self.train_loader):\n",
    "                self.dnn.train()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                y_pred, _ = self.get_residual(x)\n",
    "                _, f_u  = self.get_residual(self.train_x_f)\n",
    "\n",
    "                loss_u = torch.mean((y - y_pred) ** 2)\n",
    "                loss_f = torch.mean(f_u ** 2)\n",
    "\n",
    "                b_loss = self.boundary_loss()\n",
    "\n",
    "                loss = 100 * loss_u + loss_f + b_loss\n",
    "                loss.backward(retain_graph = True)\n",
    "                self.Adam_optim.step()\n",
    "\n",
    "            self.iter += 1\n",
    "            if self.iter % 100 == 0:\n",
    "                u_pred, f_pred = model.predict(X)\n",
    "                error_u = np.linalg.norm(U-u_pred,2)/np.linalg.norm(U,2)\n",
    "                print('Iter %d, Error u: %e' % (self.iter, error_u)) \n",
    "\n",
    "\n",
    "    def train_2(self):\n",
    "        self.dnn.train()\n",
    "        self.optimizer.step(self.loss_func)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
    "        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval()\n",
    "        u = self.net_u(x, t)\n",
    "        f = self.net_f(x, t, u)\n",
    "        u = u.detach().cpu().numpy()\n",
    "        f = f.detach().cpu().numpy()\n",
    "        return u, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38e199a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "N_u = 200\n",
    "N_i = 128\n",
    "N_f = 10000\n",
    "\n",
    "# N_u = 200\n",
    "# N_i = 100\n",
    "# N_f = 20000\n",
    "\n",
    "layers = [2, 128, 128, 128, 128, 1]\n",
    "\n",
    "data = scipy.io.loadmat('./AC.mat')\n",
    "\n",
    "t = data['tt'].flatten()[:,None] # (201,1)\n",
    "x = data['x'].flatten()[:,None]  # (512,1)\n",
    "Exact = np.real(data['uu'])\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.T.flatten()[:,None]              \n",
    "\n",
    "X_star.shape\n",
    "\n",
    "start  = 0\n",
    "step = 50\n",
    "stop = 201\n",
    "steps_lb = np.arange(0,stop+step,step)\n",
    "steps_ub = 1 + steps_lb\n",
    "\n",
    "iterations = 10000\n",
    "N_f = 20000 \n",
    "counter = 0\n",
    "\n",
    "for i in range(0,steps_lb.size-1):\n",
    "    t1 = steps_lb[i]\n",
    "    t2 = steps_ub[i+1]\n",
    "    temp_t = t[:t2,:]\n",
    "    t = t[t1:t2,:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180fff9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
